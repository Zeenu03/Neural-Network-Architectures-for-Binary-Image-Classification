{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten,Input\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Clear any logs from previous runs\n",
    "shutil.rmtree('logs', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VGG1():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(200, 200, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # vgg1_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/vgg1_callback_dir\" + str(time.time()), update_freq = 'iteration')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VGG2():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(200, 200, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = SGD(learning_rate=0.005, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VGG3():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(200, 200, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = SGD(learning_rate=0.007, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dir, test_dir, max_epochs=20):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    train_it = datagen.flow_from_directory(train_dir, class_mode='binary', batch_size=40, target_size=(200, 200))\n",
    "    test_it = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=10, target_size=(200, 200))\n",
    "    train_steps_per_epoch = len(train_it)\n",
    "    test_steps_per_epoch = len(test_it)\n",
    "    print(f\"Train steps: {train_steps_per_epoch}, Test steps: {test_steps_per_epoch}\")\n",
    "    history = {'epoch_train_loss': [], 'epoch_train_accuracy': [], 'epoch_val_loss': [], 'epoch_val_accuracy': [],\n",
    "                'iter_train_loss': [], 'iter_train_accuracy': [], 'iter_val_loss': [], 'iter_val_accuracy': []}\n",
    "    writer = tf.summary.create_file_writer(f\"logs/{model.name}\")\n",
    "    step = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}:\")\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_accuracy = []\n",
    "\n",
    "        for batch_index in range(train_steps_per_epoch):\n",
    "            X_batch, y_batch = next(train_it)\n",
    "            loss, accuracy = model.train_on_batch(X_batch, y_batch)\n",
    "\n",
    "            epoch_loss.append(loss)\n",
    "            epoch_accuracy.append(accuracy)\n",
    "\n",
    "            history['iter_train_loss'].append(loss)\n",
    "            history['iter_train_accuracy'].append(accuracy)\n",
    "            \n",
    "            loss_val,acc_val=model.evaluate(test_it,steps=test_steps_per_epoch,verbose=0)\n",
    "            epoch_val_loss.append(loss_val)\n",
    "            epoch_val_accuracy.append(acc_val)\n",
    "            history['iter_val_loss'].append(loss_val)\n",
    "            history['iter_val_accuracy'].append(acc_val)\n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar('Testing loss', loss_val, step=step)\n",
    "                tf.summary.scalar('Testing accuracy', acc_val, step=step)\n",
    "                tf.summary.scalar('Training loss', loss, step=step)\n",
    "                tf.summary.scalar('Training accuracy', accuracy, step=step)\n",
    "                writer.flush()\n",
    "            step+=1\n",
    "            \n",
    "        avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss)\n",
    "        avg_val_accuracy = sum(epoch_val_accuracy) / len(epoch_val_accuracy)\n",
    "        avg_train_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "        avg_train_accuracy = sum(epoch_accuracy) / len(epoch_accuracy)\n",
    "        history['epoch_train_loss'].append(avg_train_loss)\n",
    "        history['epoch_train_accuracy'].append(avg_train_accuracy)\n",
    "        history['epoch_val_loss'].append(avg_val_loss)\n",
    "        history['epoch_val_accuracy'].append(avg_val_accuracy)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_dir, test_dir):\n",
    "#     datagen = ImageDataGenerator(rescale=1.0/255.0, rotation_range=20)\n",
    "#     train_it = datagen.flow_from_directory(train_dir, class_mode='binary', batch_size=40, target_size=(200, 200))\n",
    "#     test_it = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=10, target_size=(200, 200))\n",
    "    \n",
    "#     train_steps_per_epoch = len(train_it) // train_it.batch_size\n",
    "#     test_steps_per_epoch = len(test_it) // test_it.batch_size\n",
    "#     log_dir = \"logs/vgg1_callback_dir\"\n",
    "#     vgg1_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "#     history = model.fit(train_it, steps_per_epoch=train_steps_per_epoch, \n",
    "#                         validation_data=test_it, validation_steps=test_steps_per_epoch, \n",
    "#                         epochs=20, verbose=2, \n",
    "#                         callbacks=[vgg1_callback] )\n",
    "\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_dir):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_it = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=10, target_size=(200, 200))\n",
    "    _, acc = model.evaluate(test_it, steps=len(test_it), verbose=2)\n",
    "    print(f'Accuracy: {acc*100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augmentation(model,train_dir, test_dir):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    train_it = datagen.flow_from_directory(train_dir, class_mode='binary', batch_size=40, target_size=(200, 200))\n",
    "    test_it = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=10, target_size=(200, 200))\n",
    "    train_steps_per_epoch = len(train_it) // train_it.batch_size\n",
    "    test_steps_per_epoch = len(test_it) // test_it.batch_size\n",
    "    history = model.fit(train_it, steps_per_epoch=train_steps_per_epoch, validation_data=test_it, validation_steps=test_steps_per_epoch, epochs=20, verbose=2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def summarize_diagnostics(history, txt):\n",
    "    # Plot two subplots one of accuracy and other of loss for iterations\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history['iter_train_loss'], color='blue', label='train')\n",
    "    plt.plot(history['iter_val_loss'], color='orange', label='test')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history['iter_train_accuracy'], color='blue', label='train')\n",
    "    plt.plot(history['iter_val_accuracy'], color='orange', label='test')\n",
    "    plt.show()\n",
    "    # Plot two subplots one of accuracy and other of loss for epochs\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history['epoch_train_loss'], color='blue', label='train')\n",
    "    plt.plot(history['epoch_val_loss'], color='orange', label='test')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history['epoch_train_accuracy'], color='blue', label='train')\n",
    "    plt.plot(history['epoch_val_accuracy'], color='orange', label='test')\n",
    "    plt.show()\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(txt+ \"_plot.png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def summarize_diagnostics(history, txt):\n",
    "#     fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\n",
    "\n",
    "#     # Plot Loss\n",
    "#     axes[0].set_title('Cross Entropy Loss')\n",
    "#     axes[0].plot(history.history['loss'], color='blue', label='train')\n",
    "#     axes[0].plot(history.history['val_loss'], color='orange', label='test')\n",
    "#     axes[0].legend(['train', 'test'], loc='upper right')\n",
    "#     axes[0].set_ylabel('Loss')\n",
    "#     axes[0].set_xlabel('Epoch')\n",
    "\n",
    "#     # Plot Accuracy\n",
    "#     axes[1].set_title('Classification Accuracy')\n",
    "#     axes[1].plot(history.history['accuracy'], color='blue', label='train')\n",
    "#     axes[1].plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "#     axes[1].legend(['train', 'test'], loc='upper right')\n",
    "#     axes[1].set_ylabel('Accuracy')\n",
    "#     axes[1].set_xlabel('Epoch')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(txt + '_plot.png')\n",
    "#     plt.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from matplotlib import pyplot\n",
    "from skimage.transform import resize\n",
    "def plot_to_image(figure):\n",
    "    buf = io.BytesIO()\n",
    "    pyplot.savefig(buf, format='png')\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    pyplot.close(figure)\n",
    "    buf.seek(0)\n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "def image_grid(predictions, images):\n",
    "    figure = pyplot.figure(figsize=(10, 10))\n",
    "    num_images = len(images)\n",
    "    permutation = np.random.permutation(num_images)\n",
    "    \n",
    "    # Shuffle images and predictions based on the permutation\n",
    "    shuffled_images = [images[i] for i in permutation]\n",
    "    shuffled_predictions = [predictions[i] for i in permutation]\n",
    "    for i in range(40):\n",
    "        # Start next subplot.\n",
    "        pyplot.subplot(5, 8, i + 1, title= 'Bear' if shuffled_predictions[i] < 0.5 else 'Koala')\n",
    "        pyplot.xticks([])\n",
    "        pyplot.yticks([])\n",
    "        pyplot.grid(False)\n",
    "        # Resize the image to 200x200 pixels before displaying it\n",
    "        resized_image = resize(shuffled_images[i], (200, 200))\n",
    "        pyplot.imshow(resized_image)\n",
    "\n",
    "    return figure\n",
    "\n",
    "def load_image(filename):\n",
    "     # load the image\n",
    "    img = load_img(filename, target_size=(200, 200))\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    "    # reshape into a single sample with 3 channels\n",
    "    img = img.reshape(1, 200, 200, 3)\n",
    "    # center pixel data\n",
    "    img = img.astype('float32')\n",
    "    # img = img - [123.68, 116.779, 103.939]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image(model, test_images, images):\n",
    "    file_writer = tf.summary.create_file_writer(f\"logs/{model.name}/test_images\")\n",
    "    predictions = []\n",
    "    for i in test_images:\n",
    "        prediction = model.predict(i)            #i has shape (1,200,200,3)\n",
    "        predictions.append(prediction[0][0])\n",
    "\n",
    "    figure = image_grid(predictions, images)\n",
    "\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.image(\"Test Image\", plot_to_image(figure), step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test_images\n",
    "test_images = []\n",
    "images = []\n",
    "# load test_images from dataset_koala_vs_bear/test/koala\n",
    "for file in os.listdir('dataset_koala_vs_bear/test/koala/'):\n",
    "    img= load_image('dataset_koala_vs_bear/test/koala/' + file)\n",
    "    images.append(Image.open('dataset_koala_vs_bear/test/koala/' + file))\n",
    "    test_images.append(img)\n",
    "\n",
    "\n",
    "for file in os.listdir('dataset_koala_vs_bear/test/bear/'):\n",
    "    img= load_image('dataset_koala_vs_bear/test/bear/' + file)\n",
    "    images.append(Image.open('dataset_koala_vs_bear/test/bear/' + file))\n",
    "    test_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(test_images))\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the table\n",
    "# Model | Train Accuracy | Test Accuracy | Train Time | Test Time\n",
    "train_accuracy = []\n",
    "train_loss = []\n",
    "test_accuracy = []\n",
    "train_time = []\n",
    "num_params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "Train steps: 4, Test steps: 4\n",
      "Epoch 1/20:\n",
      "Epoch 2/20:\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000014754926840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000014754926840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m vgg1 \u001b[38;5;241m=\u001b[39m VGG1()\n\u001b[0;32m      2\u001b[0m start_time_vgg1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgg1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset_koala_vs_bear/train/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset_koala_vs_bear/test/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m end_time_vgg1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m time_vgg1_train \u001b[38;5;241m=\u001b[39m end_time_vgg1 \u001b[38;5;241m-\u001b[39m start_time_vgg1\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dir, test_dir, max_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter_train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     25\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter_train_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m---> 26\u001b[0m loss_val,acc_val\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m epoch_val_loss\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[0;32m     28\u001b[0m epoch_val_accuracy\u001b[38;5;241m.\u001b[39mappend(acc_val)\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:410\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input/target data.\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:640\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 640\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    642\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    643\u001b[0m         dataset\n\u001b[0;32m    644\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:649\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:270\u001b[0m, in \u001b[0;36mPyDatasetAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[0;32m    267\u001b[0m         data_adapter_utils\u001b[38;5;241m.\u001b[39mNUM_BATCHES_FOR_TENSOR_SPEC,\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_dataset),\n\u001b[0;32m    269\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     batches \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39mget_tensor_spec(batches)\n\u001b[0;32m    276\u001b[0m ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator,\n\u001b[0;32m    278\u001b[0m     output_signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature,\n\u001b[0;32m    279\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:271\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[0;32m    267\u001b[0m         data_adapter_utils\u001b[38;5;241m.\u001b[39mNUM_BATCHES_FOR_TENSOR_SPEC,\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_dataset),\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     batches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_batch(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)\n\u001b[0;32m    273\u001b[0m     ]\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39mget_tensor_spec(batches)\n\u001b[0;32m    276\u001b[0m ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator,\n\u001b[0;32m    278\u001b[0m     output_signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature,\n\u001b[0;32m    279\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:68\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_index_array()\n\u001b[0;32m     65\u001b[0m index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_array[\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m idx : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m ]\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:327\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator:\n\u001b[0;32m    326\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mget_random_transform(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 327\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mstandardize(x)\n\u001b[0;32m    329\u001b[0m batch_x[i] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1413\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[1;34m(self, x, transform_parameters)\u001b[0m\n\u001b[0;32m   1410\u001b[0m img_col_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_axis \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1411\u001b[0m img_channel_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_axis \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1413\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mapply_affine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtheta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_row_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_col_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_channel_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_shift_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m     x \u001b[38;5;241m=\u001b[39m apply_channel_shift(\n\u001b[0;32m   1431\u001b[0m         x,\n\u001b[0;32m   1432\u001b[0m         transform_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_shift_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1433\u001b[0m         img_channel_axis,\n\u001b[0;32m   1434\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1879\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   1876\u001b[0m final_affine_matrix \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1877\u001b[0m final_offset \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m-> 1879\u001b[0m channel_images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_affine_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_channel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1890\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(channel_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1891\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(x, \u001b[38;5;241m0\u001b[39m, channel_axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1880\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1876\u001b[0m final_affine_matrix \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1877\u001b[0m final_offset \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1879\u001b[0m channel_images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1880\u001b[0m     \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_affine_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_channel \u001b[38;5;129;01min\u001b[39;00m x\n\u001b[0;32m   1889\u001b[0m ]\n\u001b[0;32m   1890\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(channel_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1891\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(x, \u001b[38;5;241m0\u001b[39m, channel_axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91942\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\ndimage\\_interpolation.py:614\u001b[0m, in \u001b[0;36maffine_transform\u001b[1;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    611\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[38;5;241m/\u001b[39mmatrix, output, order,\n\u001b[0;32m    612\u001b[0m                          mode, cval, npad, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 614\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometric_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg1 = VGG1()\n",
    "start_time_vgg1 = time.time()\n",
    "history = train(vgg1,'dataset_koala_vs_bear/train/', 'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg1 = time.time()\n",
    "train_time_vgg1 = end_time_vgg1 - start_time_vgg1\n",
    "train_time.append(train_time_vgg1)\n",
    "print(f\"Time taken for VGG1: {train_time_vgg1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "write_image(vgg1, test_images, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(train , steps=len(test_images), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 40961153\n",
      "Training Loss: 20\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = history.history['loss'], history.history['accuracy']\n",
    "test_loss, test_accuracy = history.history['val_loss'], history.history['val_accuracy']\n",
    "# Add scalers to tensorboard like training loss, training accuracy, testing accuracy\n",
    "# Add graphs to tensorboard like training loss vs epochs, training accuracy vs epochs, testing accuracy vs epochs\n",
    "# tf.summary.scalar('Training Loss', train_loss, step=0)\n",
    "\n",
    "num_params = vgg1.count_params()\n",
    "\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "print(f\"Training Loss: {len(train_loss)}\")\n",
    "summarize_diagnostics(history, 'vgg1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n",
      "4/4 - 1s - 155ms/step - accuracy: 0.6500 - loss: 0.5403\n",
      "Accuracy: 64.99999761581421\n",
      "Time taken for training VGG1: 93.48743796348572\n",
      "Time taken for testing VGG1: 0.8952891826629639\n"
     ]
    }
   ],
   "source": [
    "start_time_vgg1_test = time.time()\n",
    "test(vgg1,'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg1_test = time.time()\n",
    "time_vgg1_test = end_time_vgg1_test - start_time_vgg1_test\n",
    "print(f\"Time taken for training VGG1: {time_vgg1_train}\")\n",
    "print(f\"Time taken for testing VGG1: {time_vgg1_test}\")\n",
    "summarize_diagnostics(history,'vgg1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.4750 - loss: 4.5228 - val_accuracy: 0.5000 - val_loss: 0.7120\n",
      "Epoch 2/20\n",
      "4/4 - 6s - 2s/step - accuracy: 0.5000 - loss: 0.7117 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/20\n",
      "4/4 - 6s - 2s/step - accuracy: 0.5562 - loss: 0.6841 - val_accuracy: 0.5000 - val_loss: 0.6940\n",
      "Epoch 4/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.5437 - loss: 0.6788 - val_accuracy: 0.5500 - val_loss: 0.6776\n",
      "Epoch 5/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.5750 - loss: 0.6597 - val_accuracy: 0.5500 - val_loss: 0.6626\n",
      "Epoch 6/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7063 - loss: 0.6295 - val_accuracy: 0.5750 - val_loss: 0.6518\n",
      "Epoch 7/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6938 - loss: 0.6009 - val_accuracy: 0.6250 - val_loss: 0.6159\n",
      "Epoch 8/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6687 - loss: 0.5760 - val_accuracy: 0.7000 - val_loss: 0.5968\n",
      "Epoch 9/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8000 - loss: 0.5164 - val_accuracy: 0.8000 - val_loss: 0.5427\n",
      "Epoch 10/20\n",
      "4/4 - 6s - 2s/step - accuracy: 0.8062 - loss: 0.4792 - val_accuracy: 0.7750 - val_loss: 0.4854\n",
      "Epoch 11/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8125 - loss: 0.4652 - val_accuracy: 0.8000 - val_loss: 0.4744\n",
      "Epoch 12/20\n",
      "4/4 - 6s - 2s/step - accuracy: 0.7625 - loss: 0.4531 - val_accuracy: 0.6250 - val_loss: 0.5832\n",
      "Epoch 13/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7625 - loss: 0.4634 - val_accuracy: 0.8000 - val_loss: 0.5868\n",
      "Epoch 14/20\n",
      "4/4 - 6s - 2s/step - accuracy: 0.8438 - loss: 0.3824 - val_accuracy: 0.7500 - val_loss: 0.5313\n",
      "Epoch 15/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8313 - loss: 0.3645 - val_accuracy: 0.7500 - val_loss: 0.5515\n",
      "Epoch 16/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8625 - loss: 0.3537 - val_accuracy: 0.8000 - val_loss: 0.5136\n",
      "Epoch 17/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8500 - loss: 0.3200 - val_accuracy: 0.8000 - val_loss: 0.4804\n",
      "Epoch 18/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8875 - loss: 0.2797 - val_accuracy: 0.8250 - val_loss: 0.5017\n",
      "Epoch 19/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8938 - loss: 0.2557 - val_accuracy: 0.8000 - val_loss: 0.5867\n",
      "Epoch 20/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8813 - loss: 0.2755 - val_accuracy: 0.7750 - val_loss: 0.6252\n",
      "Time taken for VGG2: 135.56492972373962\n"
     ]
    }
   ],
   "source": [
    "vgg2 = VGG2()\n",
    "start_time_vgg2 = time.time()\n",
    "history = train(vgg2,'dataset_koala_vs_bear/train/', 'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg2 = time.time()\n",
    "time_vgg2 = end_time_vgg2 - start_time_vgg2\n",
    "print(f\"Time taken for VGG2: {time_vgg2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - 281ms/step - accuracy: 0.8000 - loss: 0.6288\n",
      "Accuracy: 80.0000011920929\n",
      "Time taken for testing VGG2: 1.5418579578399658\n"
     ]
    }
   ],
   "source": [
    "start_time_vgg2_test = time.time()\n",
    "test(vgg2,'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg2_test = time.time()\n",
    "summarize_diagnostics(history,'vgg2')\n",
    "time_vgg2_test = end_time_vgg2_test - start_time_vgg2_test\n",
    "print(f\"Time taken for testing VGG2: {time_vgg2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "4/4 - 9s - 2s/step - accuracy: 0.4812 - loss: 4.2923 - val_accuracy: 0.5250 - val_loss: 0.7028\n",
      "Epoch 2/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.4938 - loss: 0.6966 - val_accuracy: 0.6000 - val_loss: 0.6911\n",
      "Epoch 3/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.5375 - loss: 0.7138 - val_accuracy: 0.4750 - val_loss: 0.7205\n",
      "Epoch 4/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.4938 - loss: 0.7135 - val_accuracy: 0.6750 - val_loss: 0.6747\n",
      "Epoch 5/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.5813 - loss: 0.6762 - val_accuracy: 0.5000 - val_loss: 0.6831\n",
      "Epoch 6/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.5750 - loss: 0.6675 - val_accuracy: 0.6250 - val_loss: 0.6710\n",
      "Epoch 7/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6562 - loss: 0.6567 - val_accuracy: 0.6750 - val_loss: 0.6522\n",
      "Epoch 8/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6750 - loss: 0.6344 - val_accuracy: 0.7250 - val_loss: 0.6301\n",
      "Epoch 9/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6375 - loss: 0.6198 - val_accuracy: 0.6250 - val_loss: 0.6105\n",
      "Epoch 10/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.6500 - loss: 0.6254 - val_accuracy: 0.5500 - val_loss: 0.6432\n",
      "Epoch 11/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.6250 - loss: 0.6375 - val_accuracy: 0.7000 - val_loss: 0.5909\n",
      "Epoch 12/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7125 - loss: 0.5682 - val_accuracy: 0.7250 - val_loss: 0.5648\n",
      "Epoch 13/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7437 - loss: 0.5492 - val_accuracy: 0.7000 - val_loss: 0.5471\n",
      "Epoch 14/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7250 - loss: 0.5254 - val_accuracy: 0.7000 - val_loss: 0.5332\n",
      "Epoch 15/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.7250 - loss: 0.5216 - val_accuracy: 0.6000 - val_loss: 0.6725\n",
      "Epoch 16/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6875 - loss: 0.5577 - val_accuracy: 0.6750 - val_loss: 0.5476\n",
      "Epoch 17/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7188 - loss: 0.5066 - val_accuracy: 0.7250 - val_loss: 0.4750\n",
      "Epoch 18/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7437 - loss: 0.5209 - val_accuracy: 0.6250 - val_loss: 0.6249\n",
      "Epoch 19/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7812 - loss: 0.5000 - val_accuracy: 0.7750 - val_loss: 0.4932\n",
      "Epoch 20/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8250 - loss: 0.4576 - val_accuracy: 0.7000 - val_loss: 0.5619\n",
      "Time taken for VGG3: 152.49860095977783\n"
     ]
    }
   ],
   "source": [
    "vgg3 = VGG3()\n",
    "start_time_vgg3 = time.time()\n",
    "history = train(vgg3,'dataset_koala_vs_bear/train/', 'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg3 = time.time()\n",
    "time_vgg3 = end_time_vgg3 - start_time_vgg3\n",
    "print(f\"Time taken for VGG3: {time_vgg3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - 255ms/step - accuracy: 0.7000 - loss: 0.5962\n",
      "Accuracy: 69.9999988079071\n",
      "Time taken for testing VGG3: 1.4439589977264404\n"
     ]
    }
   ],
   "source": [
    "start_time_vgg3_test = time.time()\n",
    "test(vgg3,'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg3_test = time.time()\n",
    "summarize_diagnostics(history,'vgg3')\n",
    "time_vgg3_test = end_time_vgg3_test - start_time_vgg3_test\n",
    "print(f\"Time taken for testing VGG3: {time_vgg3_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "4/4 - 9s - 2s/step - accuracy: 0.5000 - loss: 3.1542 - val_accuracy: 0.5000 - val_loss: 0.8025\n",
      "Epoch 2/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.5312 - loss: 0.7539 - val_accuracy: 0.5000 - val_loss: 0.6872\n",
      "Epoch 3/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.5125 - loss: 0.6930 - val_accuracy: 0.5000 - val_loss: 0.6785\n",
      "Epoch 4/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.5625 - loss: 0.6766 - val_accuracy: 0.5000 - val_loss: 0.6814\n",
      "Epoch 5/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6062 - loss: 0.6650 - val_accuracy: 0.7000 - val_loss: 0.6670\n",
      "Epoch 6/20\n",
      "4/4 - 8s - 2s/step - accuracy: 0.7250 - loss: 0.6445 - val_accuracy: 0.7250 - val_loss: 0.6548\n",
      "Epoch 7/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7250 - loss: 0.6321 - val_accuracy: 0.7000 - val_loss: 0.6389\n",
      "Epoch 8/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6250 - loss: 0.6282 - val_accuracy: 0.6500 - val_loss: 0.6255\n",
      "Epoch 9/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6687 - loss: 0.5933 - val_accuracy: 0.7250 - val_loss: 0.5573\n",
      "Epoch 10/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6812 - loss: 0.5862 - val_accuracy: 0.6250 - val_loss: 0.5728\n",
      "Epoch 11/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6500 - loss: 0.6057 - val_accuracy: 0.5750 - val_loss: 0.6959\n",
      "Epoch 12/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6500 - loss: 0.6194 - val_accuracy: 0.6750 - val_loss: 0.5941\n",
      "Epoch 13/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.6250 - loss: 0.6440 - val_accuracy: 0.6750 - val_loss: 0.5956\n",
      "Epoch 14/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7625 - loss: 0.5600 - val_accuracy: 0.7250 - val_loss: 0.5815\n",
      "Epoch 15/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7563 - loss: 0.5479 - val_accuracy: 0.7500 - val_loss: 0.5378\n",
      "Epoch 16/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.7875 - loss: 0.5283 - val_accuracy: 0.8000 - val_loss: 0.4798\n",
      "Epoch 17/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8000 - loss: 0.4899 - val_accuracy: 0.7250 - val_loss: 0.5353\n",
      "Epoch 18/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8250 - loss: 0.4826 - val_accuracy: 0.7750 - val_loss: 0.4598\n",
      "Epoch 19/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8188 - loss: 0.4315 - val_accuracy: 0.7500 - val_loss: 0.5246\n",
      "Epoch 20/20\n",
      "4/4 - 7s - 2s/step - accuracy: 0.8375 - loss: 0.4298 - val_accuracy: 0.7750 - val_loss: 0.5014\n",
      "Time taken for VGG3 with augmentation: 148.7647783756256\n"
     ]
    }
   ],
   "source": [
    "vgg3_aug = VGG3()\n",
    "start_time_vgg3_aug = time.time()\n",
    "history = train_augmentation(vgg3_aug,'dataset_koala_vs_bear/train/', 'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg3_aug = time.time()\n",
    "time_vgg3_aug = end_time_vgg3_aug - start_time_vgg3_aug\n",
    "print(f\"Time taken for VGG3 with augmentation: {time_vgg3_aug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - 244ms/step - accuracy: 0.7500 - loss: 0.4567\n",
      "Accuracy: 75.0\n",
      "Time taken for testing VGG3 with augmentation: 1.5705618858337402\n"
     ]
    }
   ],
   "source": [
    "start_time_vgg3_aug_test = time.time()\n",
    "test(vgg3_aug,'dataset_koala_vs_bear/test/')\n",
    "end_time_vgg3_aug_test = time.time()\n",
    "summarize_diagnostics(history,'vgg3_aug')\n",
    "print(f\"Time taken for testing VGG3 with augmentation: {end_time_vgg3_aug_test - start_time_vgg3_aug_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
